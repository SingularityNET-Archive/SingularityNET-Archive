# (WG version)

AI Ethics strategies: scoping what they should contain.

****

What should they cover, what questions should they answer?Please answer (mainly!) for the Strategy that you’re down to work on.

## 1) Research strategy (Effiom, Vasu, Clement, PeeGee, Colleen)

#### Design a research strategy and how to implement it



**It should define ...?**

****

****

****

**It should identify... ?**

****

****

****

**It should consider...?**

****

**It should cover/include ...?**



## From meeting 3rd July 2024

Esther’s slides How do we structure a plan for doing research?

The communication gap goes both ways - there can be barriers if people don’t know what we are talking about; but also if they are not using the language and terminology we are used to around AI, we might have barriers understanding what they mean

Questions around how an interviewer should behave

Considering how we make the interview process comfortable for people. (even things like what sort of physical environment an interview should be done in?)

How we find people and get their commitment to be interviewed - and what *level* of commitment is needed? Some interviews will be quite ad-hoc - fishbowl interviews at conferences, etc - so what is the minimum level of engagement we need? They need to be able to contect us, etc - but do we want more involvement from people than just a one-off interview?

What demographic info do we want to collect (about interviewee AND interviewer, cos interviewer’s demographic can make a difference), and how to collect it without making people feel uncomfortable?

Capturing where people came into the research from? (might be useful data to know if someone came from a group with a particular outlook, for example)

Organisational map or similar, to help explain who is doing the research if they ask / want to know more

2-level info - the written stuff you give interviewees to take away, and the short version you tell people verbally

Remember we’re writing the strategy at this stage, not doing the interviews (might sound backwards, but actually might mean you identify some important things)

At the *end* of the process, might want to compare what you have with existing standards and eaccepted best-practice in qualitiative research

Try roleplays to test out whether your suggested process works! Both the writers’ group and the editors’ group could try this

Importance of flexibility, and being able to change and rethink the research strategy as we go, and as new info emerges



## From Esther: More materials about research strategy!

https://docs.google.com/document/d/1xWbFrVs9qWqmG2BaNh_9q8pAeFAN8dERgvRAhIBn_Ys/edit

## 2) Knowledge management strategy (Stephen, Ayo, Onize, Sucre, Tevo)

#### Develop a strategy to organise, store, and convey information (KM strategy) to do with the project coordination

**It should define ...?**

****

****

****

**It should identify... ?**

****

****

****

**It should consider...?**

****

****

****

**It should cover/include ...?**



## From meeting on 5th July

Ai Notetaker: https://app.read.ai/analytics/meetings/01J21XAQ8PJKZNDKXACRAD5D4P?utm_source=Share_CopyLink



Look at how we organise the materials, not how we define them? Anything we collect is, ipso facto, “knowledge” (altho worth noting that often, peer research and the insights of ordinary people are not seen as “knowledge” by some)  - it’s less about classifying, and addressing what knowledge *is*, and more about looking at how we manage it.



KM strategy is primarily about: Where and how do we keep and organise the research materials?

Making info accessible

Metrics - how much info do we have? Do we have enough people to do the documentation?

Comparing diff approaches in the Ambassador program - Knowledge Base WG and Archives



Main problem to how to collect knowledge and make it accessible. Types of knowledge is relevant to the capture solution though.



For example formal research may follow academic standards ? Peer research may need to be accommodated more discursively ?



Zotero for reference management

Might tie in with research strategy’s thinking about comparing their strategy with existing best practic, and looking at it in context with what already exists in formal research

Can you easily get material out of Zotero? - yes, you can export in several formats

there’s a web-based version where you can share citations



As an AI community do we eventually want to use AI tools?



Open source tooling is important



There is little research information about ordinary people thoughts about AI ethics - a literature review probably wouldn’t reveal a lot

so we are looking, in this strategy, of how to manage, store and make accessible **primary** source material from the GEI’s research on the general public’s feelings on AI ethics.



Hoping this will merge with Foundation’s safety and ethics framework - and that we will become a place where people can come for evidence-based info on what the public thinks about AI. So one day it *could* go more towards academic research



google drive as a first stop



onboarding to what this AI ethics is? and where you can learn more about it? (this doesn’t feel so much like **this** strategy tho)



A dialogue kind of approach might be good ? eg collate academic research materials and review those sources in the community.



Start from **What artefacts are there going to be from this project** ? Make a list and see how they fall into categories?

what different sorts of info? Not just the interviews themselves, but research journals, decisions, consent forms, what sort of data analysis tools we’ll use, who owns and who can access the different interpretations that are generatedneeds to be a clear structure how we will organise



and where people can access all the interview materials - and how we anonymise

and where interviewers can access training materials

and addressing permissions and privacy

how do we make an attribution, esp if someone wants to remain anon

Reviewing the knowledge that is collected is actually part of our strategy



NB All of this will overlap with the research strategy





What happens to the material at the end (or at the end of a cycle) We are focusing on getting the data and info-collecting flowing - atm articles and other written material is a bit out of scope of * **this** * task of designing a KM strategy - but in the long term we do need to look at how e.g. journalists etc might be able to access and use the data



Really important for this strategy to cover data privacy and conversely attribution and crediting people’s IP.



It’s not just about how info is sourced, but how it is used/interpreted. AI is just interpreting the data using certain weights n measures - can people easily understand how that’s being done, what it’s sourcing, how and why hallucinations and fabrications are emerging; able to trace them?We need a baseline that says where we got the data from, and what has been applied to it.



So this might suggest that it should NOT ever be that the original source material becomes unavailable.



Double process the data - by AI and by humans - and be transparent about it



“Info might get processed by AI, after which it can’t be unprocessed. Do people understand that proper attribution is unlikely?” Is preservation of people’s IP possible? Surely this should not happen - we should always ensure the original data is available



Do we delete the original after a certain amont of time? Attribution vs privacy? Depends on the consent form



Need a baseline, so you can see what has changed

We always have to be able to trace interpretations. Keep original source material so you can always go back and a) re-analyse and b) check that conclusions drawn were valid.

There are different types of interpretation - depending on what model you apply, you’ll get different interpretations

Principle of always using more than one model/LLM



Agree to keep simple - eg what are the min needs for AI ethics research ? In my view that is 1) respect for autonomy - appropriate recognition of sources, 2) beneficence - is the knowledge accessible and 3) Justice - can the research or opinion be engaged with.



Is “how the project resources are used” (what initiatives are rewarded, and how is that decided) part of knowledge management? Probably not at this stage, but worth considering? To what level of detail would we want to log that? Transparency about who the sponsors are, and whose interests are involved in the research, may be part of it - but maybe avoid being too granualr about it?



Who’s going to maintain information?

e.g. Life cycle of documents and artefacts, such as the materials we give to people about who is conducting this research; consent forms etc and whether they need updating; how we ensure information is updated in response to concerns that might be raised - etc



NB Can put hi-level and abstract questions into another doc, things that might br too abstract for this might still be useful at some point.





NB from Tevo:

this is an research we did a while ago and it speaks of excerises just like that. There is a section on: Strategies for choosing platforms and products Which described those guiding questions you shared.













## 3) Ethics strategy (Jenn, Duke, Ese, Kenichi, Gorga)

#### Develop an ethics strategy to ensure our research is conducted in a principled way, Syncing with the AI risk management team in the Foundation



**It should define ...?**

****

****

****

**It should identify... ?**

****

****

****

**It should consider...?**

****

****

****

**It should cover/include ...?**

****

****

## From meeting 1st July

****

**Questions/comments before the meeting started**

Does this strategy need to be a) a top-level brief statement of our research ethics for this study, like a précis or TL:DNR, followed by b) something more detailed?

****

**Vani**

What we are doing here with this research is to collect a baseline. It should be descriptive, not prescriptive - we’re trying to find out what people DO think, not what they SHOULD think.

We are trying to find out what the general public thinks, what their concerns are, and what they feel the issues are for AI ethics, regardless of whether they are well-informed, hostile, positive, anti, or whatever their view is.

****

**Esther: 5 stages**

Can look at it in terms of 5 stages

Planning, Sampling, Data collection,  Data analysis, Reporting/ Presenting

Planning

Sampling - sampling strategy to make sure people are represented equally and fairly

Data collection

Data analysis (and managing how insights are stored)

Reporting - making sense of it and representing other people’s voices



There will be overlap with the general research strategy

****

**LeeLoo: Things to involve in ethical interviewing:**

Neutrality

Baseline understanding of issues involved (which we all have mostly)

Consideration to multiple views (which could be strong and intense without being reactionary)

****

****

**Gorga: translation** I think we can create a research strategy regarding machine translation using AI from English into the languages ​​needed by people and communities. I think it is really needed when someone joins our conversation and can understand the language we speak when using it

****

**Jenn: defining the overall ethics of sNET?** Maybe we don’t have to?

Maybe we can still do ethical research even when the organisation’s position is not totally defined, or is plural?



**Kenichi**

Risk - how do we convey information about AI risk to people?

We (in this part of the work anyway) are not really doing this. We’re not educating people - we are trying to find out what they think. AI ethics isn’t a settled field yet and it’s not resolved, so we couldn’t teach someone anyway - we are more about hearing what people’s concerns are, so that maybe those concerns get fed into our understanding of what risks exist.

There’s no need to come to a definition of ethics, or educate people. Hearing that people do not know much, and knowing that is a valid outcome of the research.

We may need to create materials to neutrally explain the basics of AI ethics if people ask But writing these isn’t the task of this group - maybe” identifying that it’s needed” is part of the task, but writing/creating those resources would be something else.



Decisions that the general strategy group will probably look at:

How do we select who we interview?

Through what media will we interview people

and this group might need to liaise with them Sampling is necessarily an ethical question, since it raises questions about inclusion and vulnerability

Issues of permission



We are starting from first principles;

creating a draftand then can compare it with existing accepted standards of how to do an ethical study





## Ideas from the GEI version:



**How long should the doc be?**

Not so long that it is inaccessible



**It should cover information and informed consent**

Describe the study and who is running it  - ideally a snippet that we can re-use in many context, with all the right links

Describe its aims - ideally a snippet that we can re-use in many contexts

Have a clear, brief, accessible top-level ethics statement that can be re-used in different contexts

Have a more detailed ethics statement for those wanting to know more (layered info)

Have contact details so people know where to direct questions

What do we need to have in place in order to conduct it ethically? (e.g. consent forms; trained interviewers; clarity on where material will be held and who can access it)



**It should cover/define good practice when interviewing**

what the space should be like

how interviewers should act

what interviewers should tell interviewees before, during, and after interviews





Notes from WG meeting: Things to involve in ethical interviewing:

Neutrality

Baseline understanding of issues involved (which we all have mostly)

Consideration to multiple views (which could be strong and intense without being reactionary)





Phases of the research:







**Defining the scope of a research ethics strategy:**

How will the research be done in an ethical way? What ethical issues in research might arise in each stage of the research?













Neutrally explaining the premise (what is AI; what could ethics mean? E.g. what do we want AI to do; what do we think it shouldn’t do)

