#### Context - Do no harm
Benefits all of humanity, not just segments or groups
> * Clement Umoh: I'd also wanna add that as much as we anticipate beneficial outcomes from this, we should also braceÂ up potential issues. Now, it would be important for us to discussÂ Redress and Remedies which generally involves providing mechanisms for individuals affected by our Archive's AI decisions to seek redress, including access to effective remedies and compensation for harm if need be. :-)
>   - Vanessa Cardui: ouff - It's a good point to raise, but the Archives isn't ever likely to be in a position to offer compensation to people who feel they have been harmed. 
I think our approach to this would **have** to be clear disclaimers /waivers, and clear "here be dragons" outlines of the risks, plus a good-faith attempt to make it as transparent as possible how AI is being used
> 
#### Context - Respect for Beings, Sentient and non Sentient persons
Open Source and Creative Commons licensing
GDPR
> * Clement Umoh: Beyond this scope. I would suggest that we also include consent considerations around: (i)Â Third-Party Sharing (Inform individuals if their data will be shared with third parties and obtain separate consent for such sharing, unless sharing is necessary for the fulfillment of the original purpose). This may also include obtaining information onÂ the duration for which their consent is valid and ensure that individuals are informed if there are any changes in how their data will be processed.

ii. Cross-Border Data Transfers:
If we intend to transfer our Archives data across other systems, do we need to ensure compliance with relevant data protection laws and obtain consent if necessary for such transfers?
> 
#### Context - We need to observe what questions get asked and ask why.
> * Vanessa Cardui: Relates to Colleen's point above, about recording all human-AI interactions.

Feels like a 2-step process - record them, then classify them somehow to understand what TYPES of questions they are. Maybe classify by what (and how many) sources an AI needs to use in order to answer them? Some questions, it will just need to look at 1 line in 1 meeting summary, in isolation; other questions, it'll need to combine sources. Quite a different approach for the AI - and interesting if it's needing to look at (for example) 3 different summaries by 3 different people with 3 different sets of biases...
> 
#### Context - But our intent may be to start with narrowly
> * Colleen Pridemore: I am just curious here.Â  Why so narrow?Â  Just because of the situation and the archives growing larger with a fast growing community; hence more data?
>   - Vanessa Cardui: Clarity and relevance, really. Constraining (e.g. via a RAG retrieval process) the sources that your AI can draw on (for example, in our case, constraining it to the Archives corpus itself, plus maybe linked docs) seems likely to give more accurate and relevant answers, and fewer hallucinations and fabrications. But that's a hypothesis, which we would need to test - hence the whole RAG retrieval workÂ  :-)
>   - Colleen Pridemore: I see, yes in using RAG, that makes complete sense. Thanks for the explanation!
> 
#### Context - outputs
> * Colleen Pridemore: ...until such a time where consciousness begins to emerge in the AI then we will have to switch to "Guiding" AI.
> 
#### Context - Culture

What is your culture ?

What is the intent of your use of AI ? 

Your Ethos ?

Is it a consent based process ?
> * Jenni Illona Mayshiogie: I hope for intense discussions about this discourse ðŸ™
>   - Colleen Pridemore: ME too Jenni.Â  I believe the intent of our use of AI, like the ChatGPTÂ  AI Ethics Snet Ambassadors we will use for education both internally and with the rest of the community and ultimately, with the world.
>   - Vanessa Cardui: Same, Jenn.

I am really interested to think how it specifically applies to archives and record-keeping, since that's what this specific doc is about. I'll give it some thought, but wondered if you had any input?

(One more general question I have is - how can we expect organisations in general - but especially big corporations - to be honest about the **intent** of their use of AI? Corporations lie about their intent all the time...)
>   - Colleen Pridemore: My latest musings are within the document.  Now, there is another document
which I believe you, (Vani) posted in Discord.  Will these two be "married"
together?

I've attached it below.

 (WG version) AI Ethics strategies: scoping what they should contain
<https://docs.google.com/document/d/1eHe5PmGPpjlX7VpCxQECOHwXDEl5RRnQ1jVyUhus1tg/edit?usp=drive_web>
>   - Vanessa Cardui: Hi Colleen - no, they won't be brought together at this point, as they're for really different purposes - altho ofc they might inform each other, if some of the same people are involved in both.

This doc is for Archives WG, and is about the ethics of how **this group** uses AI, and the ethics of using AI in archiving and recordkeeping generally. It's quite a specific field, and raises some really specific questions - so it would be good to keep that clarity and  focus.

The other doc you've linked is about the GEI, and about the ethics of conducting interviews with the general public about AI. Again, a quite specific field, although of course similar things will come up.
But I think it's important not to try to converge them at this stage into a generic "soup" of AI ethics issues. I think we at least need to finish working on them before we try to merge them! 
What do you reckon?
>   - Colleen Pridemore: Makes complete sense to me.  One is for Snet AI Ethics Workgroup and One is for GEI.  I guess I had a difficult time because they started out with the same wording.
>   - Vanessa Cardui: not exactly... (sorry for the lack of clarity)
one (the one you linked) is for GEI/AI Ethics group;
and one (this one) is for Archives WG :-)
> 
#### Context - AI
> * Martin Soki35: My approach ensures that AI systems support rather than replace human decision-making, aligning with contemporary ethical standards. Additionally, I address AI risks such as hallucinations, bias, and security issues, with mitigations through explainability, cultural awareness, and education. This proactive stance will foster trustworthy and accountable AI systems, contributing to ethical, fair, and beneficial outcomes for all stakeholders.
>   - Vanessa Cardui: Hi Martin - we already have your first point, under the heading "Human-centredness".

Would you like to add your other points to the section below on "risk and mitigation", maybe?
>   - Martin Soki35: Hallucinations
   Risk: AI can give wrong information.
   Mitigation: Always provide sources for answers.

Bias
   Risk: AI can be biased.
   Mitigation: Regularly check for bias and be mindful of different cultures.

Consent
   Risk: Issues with copyright and correct credit.
   Mitigation: Ensure sources are properly credited and licensed.

Security
   Risk: Data misuse and manipulation.
   Mitigation: Educate users on secure practices and protect data.
> 
#### Context - Attribution and copyright issues
> * Vanessa Cardui: I think "consent" might cover more than just attribution and copyright.

Is there something here about consent to AI being used at all in a service? Like, a "right to refuse"? Should people be able to say, for example, "I don't want to be assessed by AI for a bank loan" or "I don't want my doctor to use AI to diagnose me" or "I want access to a non-AI search tool on this website" or whatever? Obvs could be hugely impractical - but do/should people have that right? or maybe, should they have it in some contexts?
This prolly connects with the idea of transparency - **knowing** when AI is being used and when it's not - but it occurs to me that transparency is a bit toothless if poeple cannot also object/refuse.
> 
#### Context - Hallucinations
> * Colleen Pridemore: I heard MeTTa is working on LLM Hallucinations?
> 
#### Context - Recognising merit
> * Vanessa Cardui: who defines "merit"? and how is the definition shared?

Is there maybe something here about skill or expertise?
>   - Colleen Pridemore: merit still needs clarification.Â  skills and expertise need to be evaluated to be included within the definition, "merit". Does merit imply the weight or impact their input has had on the project?
> 
#### Context - Consent
> * Vanessa Cardui: Is there also something here about AI transparency: i.e. that Archives users should be made aware of when AI is being used e.gÂ  as part of a search tool they are using, or to tag or catalogue material they are looking at? It's not exactly "consent" if there is not an alternative - but maybe there will be? (Resolved by Vanessa Cardui)
>   - Vanessa Cardui: I'd also like to include something on what I would call clarity or honesty or non-deceptiveness - we will not use AI that pretends to be human or to have emotions, or that tries to deceive humans into believing they are ineracting with a human or human-like entity, or arrogatesÂ  human functions to itself.

I dunno what people think about this - but it's one of the things I loathe about ChatGPT - it's always pretending to be human. If I ran the world, lol, AI chat bots would be banned even from saying "I feel", and they would definitely be banned from saying "I understand your feelings" or "I will strive to improve ". It's deceptive, and it gives people a mistaken impression of what AI tools actually are.
>   - Colleen Pridemore: I concurÂ completely
> 
